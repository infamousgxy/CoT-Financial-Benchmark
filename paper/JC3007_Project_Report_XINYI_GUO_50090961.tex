\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{url}

\geometry{a4paper, margin=1in}

\title{An Empirical Comparison of Direct and Explicit Reasoning Paradigms in Large Language Models on Financial Tasks}

\begin{document}

\maketitle

\vspace{10em}
\begin{center}
\noindent
{\large \textbf{JC3007: Scientific Research Methods (2025-2026) \\
Assessment: A2. Project Report}}
\end{center}

\vspace{5em}

\begin{center}
\noindent
{\large Student Name: XinYi Guo \\
ID: 50090961 \\}
\end{center}

\vfill
\begin{center}
\noindent
{\Large \textbf{University of Aberdeen}}
\end{center}

\pagebreak

\section{Introduction}

The rapid development of AIGC (AI Generated Content) is fundamentally reshaping the financial industry. In Financial Field, Large Language Models (LLMs) are now used in quantitative trading and financial customer services. Becasue their black-box nature and potential for inaccurate reasoning pose significant risks. It's not clear that explicit reasoning models fine-tuned through Chain-of-Thought (CoT) can better solve these issues than traditional instruction-tuned models. In finance Area, transparent reasoning is a necessary component of AI models’ outputs, not just correct answers. This is why this study establishes three objectives:

\begin{enumerate}
    \item Evaluate two model paradigms---instruction-tuned and thinking-type models---using five subtasks from PIXIU benchmark.
    \item Compare accuracy by five tasks to find if thinking-type models perform better on  some certain tasks (H1 hypothesis).
    \item Build an evaluation framework for logical reasoning and computational precision to evaluate the reasoning quality of both models (H2 hypothesis).
\end{enumerate}

Furthermore, this study proposes two hypotheses:

\textbf{H1 (Performance):} (a) On quantitative tasks (like FinQA, ConvFinQA), the explicit reasoning model will show significantly higher accuracy. (b) On non-quantitative tasks (like sentiment analysis, news classification), the generalist model will perform equally well or even better. This follows from specialization-generalization trade-offs.

\textbf{H2 (Reasoning Fidelity):} On quantitative tasks, reasoning traces from the explicit reasoning model will show greater logical coherence and computational accuracy compared to traces from the direct reasoning model via zero-shot CoT prompting.

\section{Related Work and Background}

PIXIU provides a comprehensive framework for evaluating financial LLMs. It covers quantitative reasoning (FinQA, ConvFinQA), sentiment analysis (FPB, FiQA-SA), and news classification (Headlines) \cite{xie2023pixiu}. As Chen et al. noted, FinQA and ConvFinQA specifically target numerical reasoning over financial tables, requiring models to extract numbers and perform multi-step calculations \cite{chen2021finqa, chen2022convfinqa}. But existing benchmarks mainly assess the accuracy of final answers. This allows models to produce correct results even when the underlying reasoning is flawed---a critical concern for verifiable decision-making in finance.

Chain-of-Thought prompting has emerged as an important technique for eliciting reasoning in LLMs. Wei et al. found that prompting models with intermediate reasoning steps substantially improves performance on arithmetic and symbolic reasoning tasks \cite{wei2022chain}. Kojima et al. also showed that LLMs can be effective zero-shot reasoners simply by adding ``Let's think step by step'' to the prompt \cite{kojima2022large}. These findings suggest that explicit reasoning processes can enhance model performance on complex tasks.

Moving to instruction tuning, Zhang et al. surveyed how to align LLMs with human preferences \cite{zhang2023instruction}. They pointed out that different fine-tuning approaches can lead to distinct model behaviors. This creates an important question: do models explicitly fine-tuned on CoT data (thinking-type models) offer advantages over standard instruction-tuned models (instruct-type models) in specialized domains like finance?

\section{Research Question}

This study investigates whether a model that is explicitly fine-tuned for reasoning outperforms a general-purpose instruction-tuned model.It focuses on both task performance and the reliability of reasoning on the PIXIU financial benchmark.

\subsection{Hypotheses}

\textbf{H1 (Performance):}
\begin{itemize}
    \item[(a)] On quantitative tasks (like FinQA, ConvFinQA), the explicit reasoning model will show significantly higher accuracy.
    \item[(b)] On non-quantitative tasks (like sentiment analysis, news classification), the generalist model will perform equally well or better.
\end{itemize}

\textbf{H2 (Reasoning Fidelity):} On quantitative tasks, reasoning traces from the explicit reasoning model will show greater logical coherence and computational accuracy compared to traces from the direct reasoning model via zero-shot CoT prompting.

\section{Experimental Design}
\label{sec:exp}

\subsection{Dataset}

This study chosses five subtasks from PIXIU benchmark: FinQA and ConvFinQA for the quantitative reasoning. The FPB and FiQA-SA for sentiment analysis and the Headlines for news classification \cite{xie2023pixiu}. We categorized these tasks into quantitative reasoning (FinQA, ConvFinQA) and qualitative analysis (Sentiment, Headlines). This can allows us to comprehensively evaluate performance differences between instruction-tuned and explicit reasoning paradigms in finance. Table~\ref{tab:dataset} shows the dataset overview.

\begin{table}[h]
\centering
\caption{Dataset Overview}
\label{tab:dataset}
\begin{tabular}{llrr}
\toprule
\textbf{Task} & \textbf{Type} & \textbf{Samples} & \textbf{Category} \\
\midrule
FinQA & Numerical Reasoning & 1,147 & Quantitative \\
ConvFinQA & Multi-turn Reasoning & 1,490 & Quantitative \\
FPB & Sentiment Classification & 970 & Qualitative \\
FiQA-SA & Sentiment Analysis & 235 & Qualitative \\
Headlines & News Classification & 20,547 & Qualitative \\
\midrule
\textbf{Total} & & \textbf{24,389} & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models}

We selected Qwen-30B-Instruct (as baseline) and Qwen-30B-Thinking (as experimental group). Both models come from the same pre-trained base. So we can reduce extra variables from architectural differences and get more accurate experimental results.

\subsection{Infrastructure}

We conducted experiments using four Nvidia L20 GPUs. We deployed both models by using vLLM with tensor parallelism to speed up model inference.

\subsection{H1 Experimental Design}

In this experiment, An automated evaluation program was developed using Python. For the data part, we accessed five datasets from HuggingFace with formatted output prompts. In the inference part, the evaluation program called the vLLM interface to test model performance on complete datasets which were selected form PIXIU.

When the experiment answer extraction and evaluation. It was found that CoT models often produced format errors due to over-reasoning. To address this, a dual-parser strategy was implemented:
\begin{itemize}
    \item \textbf{Parser 1}: Simulates a strict financial production environment. This parser only extracts answers which match regular expression patterns (measuring usability).
    \item \textbf{Parser 2}: Uses heuristic redundancy matching logic. The second parser can extract correct answers not only from regular expression patterns, but also from verbose reasoning chains. It can evaluate the model's true reasoning capability.
\end{itemize}

Besides, the prompt of the Instruct model was modified to require explicit reasoning output. This can give an ``Instruct CoT'' variant for comparing performance between explicitly requested reasoning and traditional direct output modes.

\subsection{H2 Experimental Design}

Based on the quantitative results of H1, stratified purposive sampling was employed to further analyze the underlying causes of performance differences. A total of 40 samples were drawn from each of the 5 tasks (200 total). Those samples were categorized into four types based on model correctness: Instruct-only correct, Thinking-only correct, both correct, and random samples. Quotas were designed according to performance differences across tasks. For quantitative tasks, we kept bidirectional comparison samples. For qualitative tasks (where Thinking showed significant advantages), we used random samples to supplement coverage. A fixed random seed (seed=42) ensures reproducibility.

In collaboration with two finance professionals, a qualitative evaluation rubric was developed. It covers logical coherence, computational accuracy, and factual traceability. The two evaluators independently scored samples under a double-blind protocol (with model identities concealed). This helps minimize subjective bias. Table~\ref{tab:scoring} presents the scoring criteria.

\begin{table}[h]
\centering
\caption{H2 Reasoning Quality Scoring Criteria}
\label{tab:scoring}
\begin{tabular}{p{3cm}p{8cm}c}
\toprule
\textbf{Dimension} & \textbf{Definition} & \textbf{Scale} \\
\midrule
Logical Coherence (LC) & Whether reasoning is smooth, non-redundant, and without logical gaps & 1-5 \\
Factual Fidelity (FF) & Whether faithful to source text with accurate citations & 1-5 \\
Execution Precision (EP) & Whether final reasoning is correct with sufficient justification (max 3 if incorrect) & 1-5 \\
\bottomrule
\end{tabular}
\end{table}

\section{Results}
\label{sec:results}

\subsection{H1 Results: Performance Comparison}

Figure~\ref{fig:h1results} shows the accuracy comparison across five tasks for three model configurations.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{h1_results.pdf}
\caption{H1 Performance Results: Accuracy Comparison Across Five Tasks}
\label{fig:h1results}
\end{figure}

\subsubsection{Key Finding: Thinking Model Output Format Issues}

During the analyzed error outputs, it can clearly find that approximately 71.5\% of FinQA errors happened because the Thinking model failed to output the required closing tag \texttt{</think>}. Also, 35.4\% of errors were correct calculations not placed in the ``Answer:'' format. This shows that the Thinking model's underperformance comes not from reasoning capability deficits but from execution gaps in output formatting.

\subsection{H2 Results: Reasoning Quality Evaluation}

Figure~\ref{fig:h2results} shows the reasoning quality scores from two finance professionals.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{h2_results.pdf}
\caption{H2 Reasoning Quality Scores (Averaged Across Two Evaluators)}
\label{fig:h2results}
\end{figure}

The results show that Instruct CoT model achieved a significantly higher overall score (3.83) compared to Thinking model (3.65). The main advantage lies in Logical Coherence dimension, where Instruct CoT leads by 0.68 points.

\section{Discussion}
\label{sec:discuss}

\subsection{H1(a): Quantitative Tasks}

The original H1(a) hypothesis predicted that Thinking model would show significant performance advantages on quantitative tasks compared to Instruct model. However the experimental results from PIXIU showed difference that Thinking's 53.88\% accuracy was lower than Instruct's 56.58\%. This seems to refute H1(a).

An analysis of detailed error outputs from both FinQA and ConvFinQA models showed that the Thinking model’s underperformance was not caused by deficits in reasoning capability. It was an execution gap. Approximately 71.5\% of errors came from Thinking model failing to output the required \texttt{</think>} tag. Also, 35.4\% were correct calculations not placed in the ``Answer:'' field.

Also over-reasoning phenomenon was observed: In FinQA Sample ID 6, the model actually computed the correct numerical answer during its derivation. But it ultimately refused to output the answer due to excessive skepticism about table data reasonableness (``That must be a typo...''). This introduced unnecessary noise.

\textbf{Decisive Evidence from Instruct CoT:} After modifying the prompt to introduce CoT reasoning into the Instruct model, the accuracy on FinQA surged to 71.58\%. It represents a significant improvement of 15.00\% over the original non-reasoning version and 17.70\% over the Thinking model.

This analysis supports H1(a)'s core conclusion: reasoning steps are crucial for quantitative tasks in financial scenarios. The reason why the Thinking model fails is that its uncontrolled reasoning process leads to format output failures. Instruct CoT with controlled reasoning shows that appropriate reasoning processes significantly improve performance on financial calculation tasks.

\subsection{H1(b): Qualitative Tasks}

The original H1(b) hypothesis predicted that Instruct model would achieve significant advantages on qualitative tasks like simple sentiment classification. But experimental results indicates distinctly different performance patterns for qualitative tasks of varying complexity.

\textbf{Simple Task Over-reasoning Trap:} On simple qualitative tasks like FPB, when the CoT was introduced to Instruct model, the result accuracy dropped sharply from 79\% to 72.27\%. The result accuracy closely matches the Thinking model's 72.68\%. The outputs of both CoT-enabled models showed that over-thinking raised the probability of making incorrect judgments on neutral vocabulary and simple low-entropy classification tasks. So intuitive pattern matching in non-CoT Instruct models outperforms deep logical reasoning for simple tasks.

\textbf{Reasoning Benefits in Complex Tasks:} On the complex, high-entropy FiQA-SA task (containing slang and high noise), Instruct CoT showed a significant 4.25\% performance improvement over non-CoT Instruct. More notably, Thinking model with native reasoning architecture achieved a remarkable 82.55\% accuracy on such high-complexity tasks. When facing informal expressions like ``Timberrrr'' (implying a crash), both CoT-enhanced general models and native-thinking models could derive correct meanings. But the Instruct model can't understand the meaning of this kind of words.

\subsection{H2: Reasoning Quality in Financial Domain}

Using the quantitative scoring rubric developed by two finance professionals, we obtained scores for both Instruct CoT and Thinking models. Instruct CoT score of 3.83 was significantly higher than Thinking model's 3.65. So H2 hypothesis is not supported.

\textbf{Trade-off Between Logical Coherence and Clarity:} In the financial domain, the definition of model output quality is quite different. Financial industry are extremely high requirements for ``signal-to-noise ratio'' of model outputs. The Thinking model's repeated calculations and self-redundancy were viewed by professional evaluators as lacking logical clarity and persuasiveness. Due to its structured output constraints and linear, decisive reasoning output, Instruct model received higher scores from professional evaluators even when answers showed slight deviations (scoring 0.68 points higher on logical coherence than Thinking model). Because its derivation results are auditable.

Although Thinking model's accuracy was significantly higher than Instruct CoT model, its EP score led Instruct by only 0.21 points due to unclear derivation steps and logic. This further emphasizes that in the financial domain, professionals prefer detailed and trustworthy clear derivation processes rather than precise results lacking causal reasoning.

\section{Conclusion \& Future Work}
\label{sec:conc}

Following H1 and H2 quantitative analyses, CoT shows leading advantages across all evaluation tasks except extremely simple classification tasks. But native CoT Thinking models still need to strengthen their formatted answer output capabilities. So the current best choice for financial institutions is to write different prompts for Instruct models according to different tasks. For simple tasks, the direct answers are enough. For complex tasks, institutions can have Instruct provide derivation processes to improve accuracy. Or they can have Thinking models use external calculator tools (via MCP) to get precise results.

Besides, financial experts particularly value model reasoning processes. So in subsequent financial model training, native CoT models need to strengthen their capability for clear logical output.

\section{Reflective Analysis}

Several aspects of this experiment worked well. The ablation study design proved valuable. By adding CoT prompting to the Instruct model, we created an Instruct CoT variant. This allowed us to separate the effects of reasoning capability from native architecture. The dual-parser strategy was also effective. It helped us distinguish between production usability and true reasoning capability. Besides, using models from the same pre-trained base (Qwen-30B) reduced confounding variables from architectural differences.

However, some limitations should be acknowledged. The original proposal planned to use paired t-tests with Bonferroni correction for statistical significance testing. But this was not feasible because each model produced single evaluation results per task. Future studies should conduct multiple evaluation runs to enable proper statistical analysis. Also, the proposal intended to calculate Cohen's Kappa coefficient for inter-rater reliability in H2 evaluation. Due to time constraints, we were unable to collect sufficient samples for this analysis. This is an area that needs improvement in subsequent research.

The model selection in this experiment may have been too narrow. This experiment only used Qwen series models. Different models may have significant variations. Subsequent experiments could adopt GPT-5 and GPT-5's thinking model to increase breadth. Also, the 30B parameter size may have had some influence. This could contribute to Thinking model's failure to output in the specified format. Furthermore, the number of H2 participants was too small. Only two finance professionals participated. For more rigorous evaluation, subsequent studies need to increase the number of participating evaluators.

\section{Data and Code Availability}

All experimental data, evaluation scripts, and model outputs are publicly available at: \url{https://github.com/infamousgxy/CoT-Financial-Benchmark}

\newpage

\bibliographystyle{plain}
\bibliography{myref}

\end{document}
